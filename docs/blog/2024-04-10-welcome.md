---
slug: welcome
title: Welcome to LLM Guard
authors: [therizwan]
tags: [llm-guard, typescript, security, ai]
---

We're excited to announce the release of LLM Guard, a TypeScript library designed to help you validate and secure your LLM prompts. In today's AI-driven world, ensuring the safety and quality of prompts is more important than ever.

## What is LLM Guard?

LLM Guard is a comprehensive solution for protecting your LLM applications against common vulnerabilities and ensuring the quality of your prompts. It provides a set of powerful guards that can:

- Prevent prompt injection attacks
- Detect and remove sensitive information
- Filter out toxic or inappropriate content
- Ensure prompt relevance and quality
- And much more!

## Why LLM Guard?

As AI applications become more prevalent, the need for robust security measures grows. LLM Guard helps you:

1. **Protect Your Users**: Prevent exposure of sensitive information and inappropriate content
2. **Maintain Quality**: Ensure prompts meet your standards and requirements
3. **Save Time**: Built-in guards handle common security concerns automatically
4. **Stay Flexible**: Easy to configure and extend for your specific needs

## Getting Started

Getting started with LLM Guard is simple. Just install it using npm:

```bash
npm install llm-guard
```

And check out our [getting started guide](/docs/getting-started) to learn more.

## What's Next?

We have exciting plans for LLM Guard, including:

- More specialized guards for specific use cases
- Enhanced configuration options
- Performance optimizations
- Community contributions

Stay tuned for updates, and don't forget to star our [GitHub repository](https://github.com/therizwan/llm-guard) to show your support!

## Join the Community

We welcome contributions and feedback from the community. Feel free to:

- Report issues on GitHub
- Submit pull requests
- Share your use cases
- Join discussions

Together, we can make AI applications safer and more reliable for everyone. 